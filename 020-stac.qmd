
---
  format:
    html:
      code-link: true
---

# OpenLandMap STAC 

```{r, include=FALSE, message=FALSE, results='hide'}
ls <- c("rstac", "curl")
new.packages <- ls[!(ls %in% installed.packages()[,"Package"])]
if (length(new.packages)) {
  install.packages(new.packages, repos = "https://cloud.r-project.org")
  # temporarily up to the package's new version is available on CRAN
  remotes::install_github("rolfsimoes/rstac@b-1.0.0-beta")
}
lapply(ls, require, character.only = TRUE)
```

This tutorial will utilize the open-source package `rstac` to explore the STAC static catalog of OpenLandMap. The tutorial aims to demonstrate the functionality of `rstac` in searching for datasets within OpenLandMap's STAC static catalog. 

## Listing layers

Thanks to the STAC functionality and `rstac` package, it is possible to query directly 
which collections are available on the stac.OpenLandMap.org (Note: some layers that 
are available in STAC, might not be available in the front-end / web-GIS):

```{r}
library(rstac)
olm <- read_stac("http://s3.eu-central-1.wasabisys.com/stac/openlandmap/catalog.json")
olm
```

Users should call the `read_stac()` function to access static STAC catalog data from a specified URL into their environment. This function allows users to read any type of STAC document.

To enumerate all available collections in the OpenLandMap catalog, we can scrutinize the links entry and filter only `"child"` relations that are links to collections. We do this by providing an expression that will be evaluated for each link entry inside the documents' links field:

```{r}
links(olm, rel == "child")
```

For instance, to compile a list of layers with the `title` containing the text `"GLC"` for land cover annual time-series, we can use:

```{r}
links(olm, grepl("GLC", title))
```

Let's explore the third link, referencing the `GLC_FCS30D` annual land-cover dynamic monitoring product:

```{r}
glc <- links(olm, grepl("GLC", title))[[3]] %>% link_open()
glc
```

Now, let's list its available items. We can use the same function `links()` to access and filter links using an expression. To filter all items's links let's use `rel == "item"` expression:

```{r}
links(glc, rel == "item")
```

At this point, we didn't accessed any `item` we are just seeing the links that point to items for this collection. Despite links don't have any special field like date/time or bounding box, that we could use to filter them, we could use the OpenLandMap [file naming convention](https://openlandmap.github.io/book/#the-file-naming-convention) to be able to do a finer filtering using the `href` field before read all listed links. For example, we could use `grepl()` to find the link to a specific year:

```{r}
links(glc, rel == "item", grepl("20200101", href))
```

Now we can open it by selecting the first item and calling `link_open()` function:
```{r}
glc_2020 <- links(glc, rel == "item", grepl("20200101", href))[[1]] %>% link_open()
glc_2020
```

However, to be able to proper filter items based on its spatial/temporal attributes, we need to read the items to have access to its metadata. To open a list of links we would need to call `links()` and `link_open()` many times. We can open a set of links using the functions `read_collections()` and `read_items()`. These functions call `links()` in background to filter all links for us, and open each link in a single document. `read_items()` returns an `Items` document while `read_collections()` returns a `Collections` document. 

As we do with `links()` function, we could pass additional filter expressions to `...` parameter:

```{r}
glc_items <- read_items(glc, grepl("20200101", href))
glc_items
```



We have items for all dates:

```{r}
items_datetime(glc_items)
```

To enumerate all available assets across these items, we can run:

```{r}
items_assets(glc_items)
```


## Spatial overlay

For overlaying multiple new points with COGs, we can leverage the `rstac` function `assets_url()` to retrieve the URLs of all COG files. These URLs can then be passed to an extraction function:

```{r}
urls <- assets_url(
  items = glc_items, 
  asset_names = "lc_glc.fcs30d_c_30m_s", 
  append_gdalvsi = TRUE)
urls[1:3]
```

The `append_gdalvsi=TRUE` parameter informs the function to add the string `"/vsicurl/"` to each URLs so that GDAL can open the files.

Now, let's define an extracting function. This function can extract the values in parallel:

```{r}
extract_xy = function(lon, lat, cogs, mc.cores = 10) {
  values = parallel::mclapply(cogs, function(i) {
    point <- terra::vect(matrix(c(lon, lat), ncol = 2), crs = "EPSG:4326")
    value <- terra::extract(terra::rast(i), point)
    #dplyr::as_tibble(value)[,2]
    value[,2]
  }, mc.cores = mc.cores)
  values = dplyr::tibble(glc = unlist(values))
  return(values)
}
```

This only needs URL address of the COGs on some S3 storage and then longitude and 
latitude of the query points (in the WGS84 system). This is an example of query of 
all land cover classes from 1985 to 2022:

```{r}
values <- extract_xy(-35.5, -9.0, urls)
# add date column
values$date <- as.Date(items_datetime(glc_items))
values
```

