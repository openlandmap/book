# OpenLandMap STAC 

```{r, results = "asis", echo = FALSE}
status("drafting")
```

```{r, include=FALSE, message=FALSE, results='hide'}
ls <- c("rstac", "curl")
new.packages <- ls[!(ls %in% installed.packages()[,"Package"])]
if (length(new.packages)) {
  install.packages(new.packages, repos = "https://cloud.r-project.org")
  # temporarily up to the package's new version is available on CRAN
  devtools::install_github("rolfsimoes/rstac@b-1.0.0-beta")
}
lapply(ls, require, character.only = TRUE)
```

## Listing layers

Thanks to the STAC functionality and `rstac` package, it is possible to query directly 
which collections are available on the stac.OpenLandMap.org (Note: some layers that 
are available in STAC, might not be available in the front-end / web-GIS):

```{r}
library(rstac)
olm <- stac_read("http://s3.eu-central-1.wasabisys.com/stac/openlandmap/catalog.json")
olm
```

To enumerate all available collections in the OpenLandMap catalog, we can scrutinize the links entry:

```{r}
links(olm)
```

For instance, to compile a list of layers with the `title` containing the text `"GLC"` for land cover annual time-series, we can use:

```{r}
links(olm, grepl("GLC", title))
```

Let's explore the third link, referencing the `GLC_FCS30D` annual land-cover dynamic monitoring product:

```{r}
glc_link <- links(olm, grepl("GLC", title))[[3]]
glc <- link_open(glc_link)
glc
```

Now, let's list its available items by filtering links with the `rel == "item"` attribute:

```{r}
links(glc, rel == "item")
```

To be able to filter items based on spatial and temporal attributes, we need to open them:

```{r}
glc_items <- read_items(glc, progress = FALSE)
glc_items
```

We have items for all dates:

```{r}
items_datetime(glc_items)
```

To enumerate all available assets across these items, we can run:

```{r}
items_assets(glc_items)
```


## Spatial overlay

For overlaying multiple new points with COGs, we can leverage the `rstac` function `assets_url()` to retrieve the URLs of all COG files. These URLs can then be passed to an extraction function:

```{r}
urls <- assets_url(glc_items, asset_names = "lc_glc.fcs30d_c_30m_s", append_gdalvsi = TRUE)
urls[1:3]
```

Let's define an extracting function. This function can extract the values in parallel:

```{r}
extract_xy = function(lon, lat, cogs, mc.cores = 10) {
  values = parallel::mclapply(cogs, function(i) {
    point <- terra::vect(matrix(c(lon, lat), ncol = 2), crs = "EPSG:4326")
    value <- terra::extract(terra::rast(i), point)
    #dplyr::as_tibble(value)[,2]
    value[,2]
  }, mc.cores = mc.cores)
  values = dplyr::tibble(glc = unlist(values))
  return(values)
}
```

This only needs URL address of the COGs on some S3 storage and then longitude and 
latitude of the query points (in the WGS84 system). This is an example of query of 
all land cover classes from 1985 to 2022:

```{r}
values <- extract_xy(-35.5, -9.0, urls)
# add date column
values$date <- as.Date(items_datetime(glc_items))
values
```


